<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Linear Regression</title>
  <meta name="description" content="This is the notebook for the book “Applied Linear Regression Models”.">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/2017/06/20/Linear_Regression.html">
  <link rel="alternate" type="application/rss+xml" title="zjuwhw's blog" href="http://localhost:4000/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">zjuwhw's blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/links/">Links</a>
          
        
          
        
          
          <a class="page-link" href="/tags/">Tags</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Linear Regression</h1>
    <p class="post-meta"><time datetime="2017-06-20T00:00:00+10:00" itemprop="datePublished">Jun 20, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>This is the notebook for the book “<a href="https://www.amazon.com/Applied-Linear-Regression-Models-Student/dp/0073014664">Applied Linear Regression Models</a>”.</p>

<ul id="markdown-toc">
  <li><a href="#part-1-simple-linear-regression" id="markdown-toc-part-1-simple-linear-regression">Part 1 Simple linear regression</a>    <ul>
      <li><a href="#chapter-1-linear-regression-with-one-predictor-variable" id="markdown-toc-chapter-1-linear-regression-with-one-predictor-variable">Chapter 1 Linear regression with one predictor variable</a>        <ul>
          <li><a href="#11-relations-between-variables" id="markdown-toc-11-relations-between-variables">1.1 Relations between variables</a></li>
          <li><a href="#12-regression-models-and-their-uses" id="markdown-toc-12-regression-models-and-their-uses">1.2 Regression Models and Their Uses</a></li>
          <li><a href="#13-simple-linear-regression-model-with-distribution-of-error-terms-unspecified" id="markdown-toc-13-simple-linear-regression-model-with-distribution-of-error-terms-unspecified">1.3 Simple linear regression model with distribution of error terms unspecified</a></li>
          <li><a href="#14-data-for-regression-analysis" id="markdown-toc-14-data-for-regression-analysis">1.4 Data for regression analysis</a></li>
          <li><a href="#15-overview-of-steps-in-regression-analysis" id="markdown-toc-15-overview-of-steps-in-regression-analysis">1.5 Overview of steps in regression analysis</a></li>
          <li><a href="#16-estimation-of-regression-function" id="markdown-toc-16-estimation-of-regression-function">1.6 Estimation of regression function</a></li>
          <li><a href="#17-estimation-of-erro-terms-variance-sigma2" id="markdown-toc-17-estimation-of-erro-terms-variance-sigma2">1.7 Estimation of Erro Terms Variance $\sigma^{2}$</a></li>
          <li><a href="#18-normal-error-regression-model" id="markdown-toc-18-normal-error-regression-model">1.8 Normal Error Regression Model</a></li>
        </ul>
      </li>
      <li><a href="#chapter-2-inferences-in-regression-and-correlation-analysis" id="markdown-toc-chapter-2-inferences-in-regression-and-correlation-analysis">Chapter 2 Inferences in Regression and Correlation Analysis</a>        <ul>
          <li><a href="#21-inferences-concerning-beta_1" id="markdown-toc-21-inferences-concerning-beta_1">2.1 Inferences Concerning $\beta_{1}$</a></li>
          <li><a href="#22-inferences-concerning-beta_0" id="markdown-toc-22-inferences-concerning-beta_0">2.2 Inferences Concerning $\beta_{0}$</a></li>
          <li><a href="#23-some-considerations-on-making-inferences-concerning-beta_0-and-beta_1" id="markdown-toc-23-some-considerations-on-making-inferences-concerning-beta_0-and-beta_1">2.3 Some Considerations on Making Inferences Concerning $\beta_{0}$ and $\beta_{1}$</a></li>
        </ul>
      </li>
      <li><a href="#chapter-5-matrix-approach-to-simple-linear-regression-analysis" id="markdown-toc-chapter-5-matrix-approach-to-simple-linear-regression-analysis">Chapter 5 Matrix Approach to Simple Linear Regression Analysis</a>        <ul>
          <li><a href="#51-matrices" id="markdown-toc-51-matrices">5.1 Matrices</a></li>
          <li><a href="#52-matrix-addition-and-subtraction" id="markdown-toc-52-matrix-addition-and-subtraction">5.2 Matrix Addition and Subtraction</a></li>
          <li><a href="#53-matrix-multiplication" id="markdown-toc-53-matrix-multiplication">5.3 Matrix Multiplication</a></li>
          <li><a href="#54-special-types-of-matrices" id="markdown-toc-54-special-types-of-matrices">5.4 Special Types of Matrices</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="part-1-simple-linear-regression">Part 1 Simple linear regression</h2>

<h3 id="chapter-1-linear-regression-with-one-predictor-variable">Chapter 1 Linear regression with one predictor variable</h3>

<h4 id="11-relations-between-variables">1.1 Relations between variables</h4>

<ul>
  <li>relation
    <ul>
      <li><em>Function relation</em>: Y = f(X), e.g. total cost = the number of products * cost per product</li>
      <li><em>Statistical relation</em>: not a perfect one, e.g. performance for 10 employees at midyear and year-end</li>
    </ul>
  </li>
  <li>variable
    <ul>
      <li>X: <em>independent/explanatory/predictor variable</em></li>
      <li>Y: <em>dependent/response variable</em></li>
    </ul>
  </li>
  <li>plot
    <ul>
      <li><em>scatter diagram/plot</em></li>
      <li>each point represents a <em>trial</em> or a <em>case</em></li>
    </ul>
  </li>
</ul>

<h4 id="12-regression-models-and-their-uses">1.2 Regression Models and Their Uses</h4>

<ul>
  <li>History
    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Francis_Galton">Sir Francis Galton</a> in the latter part of 19th century</li>
      <li>relation between heights of parents and children</li>
      <li>regression to the mean</li>
    </ul>
  </li>
  <li>Basic Concepts
    <ul>
      <li>A regression model</li>
      <li>two characters:
        <ul>
          <li>there is a probability distribution of Y for each level of X</li>
          <li>The means of these probability distribution vary in some systematic fashion with X</li>
        </ul>
      </li>
      <li><em>regression function</em>: the systematic relationship
        <ul>
          <li><em>linear</em>, <em>curvilinear</em>, etc.</li>
        </ul>
      </li>
      <li><em>regression curve</em>: the graph of the regression function</li>
      <li>probability distributions: <em>symmetrical</em>, <em>skewed</em> etc.</li>
    </ul>
  </li>
  <li>Regression models with more than one predictor variable</li>
  <li>Construction of Regression Models
    <ul>
      <li>Selection of predictor variables</li>
      <li>Functional form of regression relation</li>
      <li>Scope of model</li>
    </ul>
  </li>
  <li>Uses of regression analysis
    <ul>
      <li>description</li>
      <li>control</li>
      <li>prediction</li>
      <li>overlap in practice</li>
    </ul>
  </li>
  <li>Regeression and causality</li>
  <li>Use of Computers</li>
</ul>

<h4 id="13-simple-linear-regression-model-with-distribution-of-error-terms-unspecified">1.3 Simple linear regression model with distribution of error terms unspecified</h4>

<ul>
  <li>Formal statement of model</li>
</ul>

<script type="math/tex; mode=display">Y_{i} = \beta_{0} + \beta_{1}X_{i} + \varepsilon_{i}</script>

<p>where:</p>

<ul>
  <li>$Y_{i}$ is the value of th response variable in the ith trail</li>
  <li>$\beta_{0}\text{ and }\beta_{1}$ are paramters</li>
  <li>$X_{i}$ is a known constant, namely, the value of the predictor variable in the ith trial</li>
  <li>$\varepsilon_{i}$ is a random error term
    <ul>
      <li>mean $E(\varepsilon_{i}) = 0$</li>
      <li>variance $\sigma^{2}(\varepsilon_{i}) = \sigma^{2}$</li>
      <li>covariance $\sigma(\varepsilon_{i}, \varepsilon_{j}) = 0$, for all i, j; $i \neq j$</li>
    </ul>
  </li>
  <li>Important features of model</li>
</ul>

<ol>
  <li>$Y_{i}$ contains two components: the constant term $\beta_{0} + \beta_{1}X_{i}$ and the random term $\varepsilon_{i}$. Hence, $Y_{i}$ is a random variable</li>
  <li>Since $E(\varepsilon_{i}) = 0$, $E(Y_{i}) = E(\beta_{0} + \beta_{1}X_{i} + \varepsilon_{i}) = \beta_{0} + \beta_{1}X_{i} + E(\varepsilon_{i}) = \beta_{0} + \beta_{1}X_{i}$</li>
  <li>The response $Y_{i}$ in the ith trail exceeds or falls short of the value of the regssion fucntion by the error term amount $\varepsilon_{i}$</li>
  <li>The erorr term $\varepsilon_{i}$ are assumed to have constant variance $\sigma^{2}$ and $\sigma^{2}(Y_{i}) = \sigma^{2}$</li>
  <li>The error terms are assumed to be uncorrelated, so are the responses $Y_{i}$ and $Y_{j}$</li>
</ol>

<ul>
  <li>Meaning of regression paramters
    <ul>
      <li><em>regrssion coefficients</em>: the paramters $\beta_{0}\text{ and }\beta_{1}$</li>
      <li><em>the slope of the regression line</em>: $\beta_{1}$</li>
    </ul>
  </li>
  <li>Alternative versions of regression model</li>
</ul>

<script type="math/tex; mode=display">Y_{i} = \beta_{0}X_{0} + \beta_{1}X_{i} + \varepsilon_{i}\text{, where }X_{0} \equiv 1</script>

<script type="math/tex; mode=display">Y_{i} = \beta_{0}^{*} + \beta_{1}(X_{i} - \bar{X}) + \varepsilon_{i}\text{, where }\beta_{0}^{*} = \beta_{0} + \beta_{1}\bar{X}</script>

<h4 id="14-data-for-regression-analysis">1.4 Data for regression analysis</h4>

<ul>
  <li>Observational Data</li>
  <li>Eperimental Data
    <ul>
      <li>treatment</li>
      <li>experimental units</li>
    </ul>
  </li>
  <li>Completely randomized design</li>
</ul>

<h4 id="15-overview-of-steps-in-regression-analysis">1.5 Overview of steps in regression analysis</h4>

<h4 id="16-estimation-of-regression-function">1.6 Estimation of regression function</h4>

<ul>
  <li>Methods of Least Squares
    <ul>
      <li>To find estimates $b_{0}$ and $b_{1}$ for $\beta_{0}$ and $\beta_{1}$, respectively, for which Q is a minimum, where $Q = \displaystyle\sum_{i=1}^{n}(Y_{i} - \beta_{0} - \beta_{1}X_{i})^2$.</li>
      <li>Least Squares Estimators $b_{0}$ and $b_{1}$ can be found in two ways:
        <ul>
          <li>numerical search procedures</li>
          <li>analytical procedures</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">b_{1} = \frac{\sum(X_{i} - \bar{X})(Y_{i} - \bar{Y})}{\sum(X_{i} - \bar{X})^2}</script>

<script type="math/tex; mode=display">b_{0} = \frac{1}{n}(\sum Y_{i} - b_{1} \sum X_{i}) = \bar{Y} - b_{1}\bar{X}</script>

<ul>
  <li>Proof</li>
</ul>

<p>The paritial derivatives are</p>

<script type="math/tex; mode=display">\frac{\partial Q}{\partial\beta_{0}} = -2\sum(Y_{i} - \beta_{0} - \beta_{1}X_{i})</script>

<script type="math/tex; mode=display">\frac{\partial Q}{\partial\beta_{1}} = -2\sum X_{i}(Y_{i} - \beta_{0} - \beta_{1}X_{i})</script>

<p>We set them equal to zero, using $b_{0}$ and $b_{1}$ to denote the particular values of $b_{0}$ and $b_{1}$ that minimize Q:</p>

<script type="math/tex; mode=display">-2\sum(Y_{i} - \beta_{0} - \beta_{1}X_{i}) = 0</script>

<script type="math/tex; mode=display">-2\sum X_{i}(Y_{i} - \beta_{0} - \beta_{1}X_{i}) = 0</script>

<ul>
  <li>Proerties of Least Squares Estimators</li>
</ul>

<p>Guass-Markov theorem:</p>

<blockquote>
  <p>Under the conditions of regression model, the least squares estimators b0 and b1 are unbiased and have minimum variance among all unbiased linear estimators</p>
</blockquote>

<ul>
  <li>Point Esitmation of Mean Response
    <ul>
      <li>estimate the regression function as follows:</li>
    </ul>

    <script type="math/tex; mode=display">\hat{Y} = b_{0} + b_{1}X</script>
  </li>
  <li>Residuals
    <ul>
      <li>residual: the differenc between the observed value $Y_{i}$ and the corresponding fitted value $\hat{Y_{i}}$</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">e_{i} = Y_{i} - \hat{Y}_{i}</script>

<ul>
  <li>Properties of Fitted Regression Line
    <ul>
      <li>$\sum e_{i} = 0$</li>
      <li>$\sum e_{i}^{2}$ is a minimum</li>
      <li>$\sum Y_{i} = \sum \hat{Y}_{i}$</li>
      <li>$\sum X_{i} e_{i} = 0 $</li>
      <li>$\sum \hat{Y}_{i}$e<sub>i</sub> = 0</li>
      <li>the regression line always goes through the point $(\bar{X}, \bar{Y})$</li>
    </ul>
  </li>
</ul>

<h4 id="17-estimation-of-erro-terms-variance-sigma2">1.7 Estimation of Erro Terms Variance $\sigma^{2}$</h4>

<ul>
  <li>Point Estimator of $\sigma^{2}$
    <ul>
      <li>Single population
        <ul>
          <li>sum of squares: $\displaystyle\sum_{i=1}^{n}(Y_{i}-\bar{Y})^2$</li>
          <li>degrees of freedom (df): n - 1, because one degree of freedom is lost by using $\bar{Y}$ as an estimate of the unknown population mean $\mu$</li>
          <li>sample variance/mean square: $s^2 = \frac{\displaystyle\sum(Y_{i}-\bar{Y})^2}{n-1}$</li>
        </ul>
      </li>
      <li>Regression model
        <ul>
          <li>deviation/residual: $Y_{i} - \hat{Y}_{i}$ = e<sub>i</sub></li>
          <li>error/residual sum of squares:
            <ul>
              <li>$SSE = \displaystyle\sum_{i=1}^{n}(Y_{i} - \hat{Y}_{i})^{2}$</li>
              <li>$SSE = \displaystyle\sum_{i=1}^{n}e_{i}^{2}$</li>
            </ul>
          </li>
          <li>degrees of freedom: n - 2, because two degrees of freedom are lost due to estimating $\beta_{0}$ and $\beta_{1}$ to get $\hat{Y}_{i}$</li>
          <li>MSE (error/residual mean square): $s^{2} = MSE = \frac{SSE}{n-2}$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="18-normal-error-regression-model">1.8 Normal Error Regression Model</h4>

<ul>
  <li>Model
    <ul>
      <li>same with simple linear regression model</li>
      <li>except it assumes that the error $\varepsilon_{i}$ are normally distributed</li>
    </ul>
  </li>
  <li>Estimation of Parameters by Method of Maximum Likelihood</li>
</ul>

<p>Method of maximum likelihood chooses as estimates those values of the parameters that are most consistent with the sample data.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>A normal distribution with SD = 10, mean is unknown
A random of sample n = 3 yields the results 250, 265 and 259
The likelihood value (L) is the product of the densities of the normal distribution

If we assue μ = 230, L(μ = 230) = 0.279*10E-9 
R code: prod(dnorm(c(250,265,259), 230, 10))
If we assue μ = 259, L(μ = 259) = 0.0000354
R code: prod(dnorm(c(250,265,259), 259, 10))

So, L(μ = 259) &gt; L(μ = 230)

The method of maximum likelihood is to estimate prarametes to get maximum L.
It can be shown that for a normal population,
the maximum likelihood estimator of μ is the smaple mean
</code></pre>
</div>

<p>Fro regression model, the likelihood function for n observations is</p>

<script type="math/tex; mode=display">L(\beta_{0}, \beta_{1}, \sigma^{2}) = \displaystyle\Pi_{i=1}^{n}\frac{1}{(2\pi\sigma^{2})^{1/2}}exp[-\frac{1}{2\sigma^{2}}(Y_{i} - \beta_{0} - \beta_{1}X_{i})^{2}]</script>

<script type="math/tex; mode=display">L(\beta_{0}, \beta_{1}, \sigma^{2}) = \frac{1}{(2\pi\sigma^{2})^{1/2}}exp[-\frac{1}{2\sigma^{2}}\displaystyle\sum_{i=1}^{n}(Y_{i} - \beta_{0} - \beta_{1}X_{i})^{2}]</script>

<h3 id="chapter-2-inferences-in-regression-and-correlation-analysis">Chapter 2 Inferences in Regression and Correlation Analysis</h3>

<h4 id="21-inferences-concerning-beta_1">2.1 Inferences Concerning $\beta_{1}$</h4>

<ul>
  <li>Sampling Distribution of b1</li>
</ul>

<p>The sampling distribution of b1 refers to the different values of b1 that would be obtained with repeated sampling when the levels of the predictor variable X are held constant from sample to sample.</p>

<p>For normal error regression model, the sample distributon of b1 is <strong>normal</strong>, with mean and variance:</p>

<script type="math/tex; mode=display">E(b1) = \beta_{1}</script>

<script type="math/tex; mode=display">\sigma^{2}(b1) = \frac{\sigma^{2}}{\sum(X_{i} - \bar{X})^{2}}</script>

<ul>
  <li>Proof</li>
</ul>

<p>b1 as linear combination of the Yi</p>

<p><script type="math/tex">b1 = \sum k_{i}Y_{i}\text{where }k_{i} = \frac{X_{i} - \bar{X}}{\sum(X_{i} - \bar{X})^{2}}</script>
 - Nomaily</p>

<p>The Yi are independently, normally distributed, so b1 are normally distributed.</p>

<ul>
  <li>Mean</li>
</ul>

<script type="math/tex; mode=display">E(b_{1}) = E(\sum k_{i}Y_{i}) = \sum k_{i}E(Y_{i}) = \sum k_{i}(\beta_{0} + \beta_{1}X_{i}) = \beta_{1}</script>

<p>hint:</p>

<script type="math/tex; mode=display">\sum k_{i} = 0</script>

<script type="math/tex; mode=display">\sum k_{i}X_{i} = 1</script>

<ul>
  <li>Variance</li>
</ul>

<script type="math/tex; mode=display">\sigma^{2}(b_{1}) = \sigma^{2}(\sum k_{i}Y_{i}) = \sum k_{i}^{2}\sigma^{2}(Y_{i}) = \sum k_{i}^{2}\sigma^{2} = \sigma^{2}\frac{1}{\sum (X_{i} - \bar{X})^{2}}</script>

<ul>
  <li>Estimated Variance</li>
</ul>

<p>Replace the paramter $\sigma^{2}$ with MSE:</p>

<script type="math/tex; mode=display">s^{2}(b_{1}) = \frac{MSE}{\sum(X_{i} - \bar{X})^{2}}</script>

<ul>
  <li>Sampling Distribution of $(b_{1} - \beta_{1})/s(b_{1})$</li>
</ul>

<script type="math/tex; mode=display">(b_{1} - \beta_{1})/\sigma(b_{1}) \sim N(0,1)</script>

<script type="math/tex; mode=display">(b_{1} - \beta_{1})/s(b_{1}) \sim t(n-2)</script>

<p>When a statistic is standardized but the denominator is an estimated standard deviation rather than the true standard deviation, it is called a studentized statistic.</p>

<ul>
  <li>Comment</li>
</ul>

<script type="math/tex; mode=display">SSE/\sigma^{2} \sim \chi^{2}(n - 2)</script>

<script type="math/tex; mode=display">(b_{1} - \beta_{1})/s(b_{1}) \sim \frac{z}{\sqrt{\frac{\chi^2(n-2)}{n-2}}} = t(n-2)</script>

<ul>
  <li>Confidence Interval for $\beta_{1}$</li>
</ul>

<script type="math/tex; mode=display">b_{1} \pm t(1-\alpha/2; n-2)s(b_{1})\text{ where }\alpha\text{ is significance level}</script>

<ul>
  <li>Tests concerning $\beta_{1}$</li>
</ul>

<p>Since $(b_{1} - \beta_{1})/s(b_{1})$ is ditributed as t with n - 2degrees of freedom, tests concerning $\beta_{1}$ can be set up in ordinary fashion using the t distribution.</p>

<h4 id="22-inferences-concerning-beta_0">2.2 Inferences Concerning $\beta_{0}$</h4>

<p>The sampling distribution of $\beta_{0}$ is normal, with mean and variance:</p>

<script type="math/tex; mode=display">E(b_{0}) = \beta_{0}</script>

<script type="math/tex; mode=display">\sigma^{2}(b_{0}) = \sigma^{2}[\frac{1}{n} + \frac{\bar{X}^{2}}{\sum (X_{i} - \bar{X})^{2}}]</script>

<script type="math/tex; mode=display">s^{2}(b_{0}) = MSE[\frac{1}{n} + \frac{\bar{X}^{2}}{\sum (X_{i} - \bar{X})^{2}}]</script>

<script type="math/tex; mode=display">\frac{b_{0} - \beta_{0}}{s(b_{0})} \sim t(n-2)</script>

<h4 id="23-some-considerations-on-making-inferences-concerning-beta_0-and-beta_1">2.3 Some Considerations on Making Inferences Concerning $\beta_{0}$ and $\beta_{1}$</h4>

<ul>
  <li>Effects of Departures from Normality</li>
  <li>Interpretation of Confidence Coefficient and Risks of Errors</li>
  <li>Spacing of the X levels</li>
  <li>Power of Tests</li>
</ul>

<p>The power of this test is the probability that the decision rule will lead to conclusion $H_{a}$ when $H_{a}$ in fact holds. Specifically, the power is given by</p>

<script type="math/tex; mode=display">Power = P(|t^{*}| > t(1-\alpha/2;n-2)|\delta)</script>

<p>where,</p>

<ul>
  <li>$H_{0}: \beta_{1} = \beta_{10}$; $H_{a}: \beta_{1} \neq \beta_{10}$</li>
  <li>$t^{*} = \frac{b_{1} - \beta_{10}}{s(b_{1})}$</li>
  <li>$\delta$ is the <strong>noncentrality measure</strong>, a measure of how far the true value of $\beta_{1}$ is from $\beta_{10}$. $\delta = \frac{\mid\beta_{1} - \beta_{10}\mid}{\sigma(b_{1})}$</li>
</ul>

<h3 id="chapter-5-matrix-approach-to-simple-linear-regression-analysis">Chapter 5 Matrix Approach to Simple Linear Regression Analysis</h3>

<h4 id="51-matrices">5.1 Matrices</h4>

<ul>
  <li>Definition
    <ul>
      <li>matrix</li>
      <li>dlements</li>
      <li>dimension</li>
    </ul>
  </li>
  <li>Notation: a boldface symbol, such as <strong>A</strong>, <strong>X</strong> or <strong>Z</strong>.</li>
  <li>Square Matrix: the number of rows equals the number of columns</li>
  <li>Vector
    <ul>
      <li>column vector/vector</li>
      <li>row vector</li>
    </ul>
  </li>
  <li>Transpose: <strong>A’</strong> (A prime) is the transpose of a matrix <strong>A</strong></li>
  <li>Equality of Matrices: same dimension and all same corresponding elements</li>
  <li>design matrix</li>
</ul>

<h4 id="52-matrix-addition-and-subtraction">5.2 Matrix Addition and Subtraction</h4>

<ul>
  <li>same dimension</li>
  <li>the sum or difference of the corresponding elements of the two matrixs</li>
  <li><strong>A</strong> + <strong>B</strong> = <strong>B</strong> + <strong>A</strong></li>
</ul>

<h4 id="53-matrix-multiplication">5.3 Matrix Multiplication</h4>

<ul>
  <li>Multiplication of a Matrix by a <strong>scalar</strong>
    <ul>
      <li>a scalar is an ordinary number or a symbol representing a number</li>
    </ul>
  </li>
  <li>Multiplication of a Matrix by a Matrix
    <ul>
      <li>the product <strong>AB</strong>, we say that <strong>A</strong> is postmultiplied by <strong>B</strong> or <strong>B</strong> is premultiplied by <strong>A</strong></li>
      <li>$AB \neq BA$</li>
    </ul>
  </li>
</ul>

<p>In general, if <strong>A</strong> has dimension r * c and <strong>B</strong> has dimension c * s, the product <strong>AB</strong> is a matrix of dimension r * s, which is</p>

<script type="math/tex; mode=display">AB_{r \times s} = \begin{bmatrix}\sum_{k=1}^{c} a_{ik}b_{kj}\end{bmatrix}\text{, where }i=1,...,r;j=1,...,s</script>

<ul>
  <li>Regression Examples</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
Y'Y_{1 \times 1} = \begin{bmatrix} Y_{1} & Y_{2} & ... & Y_{n} \end{bmatrix}\begin{bmatrix}Y_{1} \\ Y_{2} \\ ... \\ Y_{n} \end{bmatrix} =  Y_{1}^{2} + Y_{2}^{2} + ... + Y_{n}^{2} = \sum Y_{i}^{2} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
X'X_{2 \times 2} = \begin{bmatrix} 1 & 1 & ... & 1 \\ X_{1} & X_{2} & ... & X_{n} \end{bmatrix}\begin{bmatrix} 1 & X_{1} \\ 1 & X_{2} \\ ... \\ 1 & X_{n} \end{bmatrix} =  \begin{bmatrix} n & \sum X_{i} \\ \sum X_{i} & \sum X_{i}^{2} \end{bmatrix} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
X'Y_{2 \times 1} = \begin{bmatrix} 1 & 1 & ... & 1 \\ X_{1} & X_{2} & ... & X_{n} \end{bmatrix}\begin{bmatrix} Y_{1} \\ Y_{2} \\ ... \\ Y_{n} \end{bmatrix} =  \begin{bmatrix} \sum Y_{i} \\ \sum X_{i}Y_{i} \end{bmatrix} %]]></script>

<h4 id="54-special-types-of-matrices">5.4 Special Types of Matrices</h4>

<ul>
  <li>Symmetric Matrix: <strong>A</strong> = <strong>A’</strong>
    <ul>
      <li>Symmetric matrix is necessarily square</li>
      <li>premultiply a matrix by its transpose, say <strong>X’X</strong> is symmetric</li>
    </ul>
  </li>
  <li>Diagonal Matrix: off-diagonal elements are all zeros</li>
  <li>Identity Matrix, denoted by <strong>I</strong>: a diagonal matrix whose elements on the main diagonal are all 1s.
    <ul>
      <li><strong>AI</strong> = <strong>IA</strong> = <strong>A</strong></li>
    </ul>
  </li>
  <li>Scalar Marix: k<strong>I</strong></li>
</ul>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">


    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>zjuwhw's blog</li>
          <li><a href="mailto:zju_whw@163.com">zju_whw@163.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/zjuwhw"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">zjuwhw</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/zjuwhw"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">zjuwhw</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>This is zjuwhw's personal blog. Powered by Jekyll and Host in Github.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
